{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports of utility functions we will be using\n",
    "below we import the work for our residual network and deep hybrid scatter network. All the real 'heavy lifting' for this project was done in the resnet and ScatterTransform scripts. For a more detailed look at what's going on under the hood go give the scripts a peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/ubuntu/feature_viz/resnet')\n",
    "sys.path.append('/home/ubuntu/feature_viz/ScatteringTransform/src/model')\n",
    "from flags import define_flags as scatternet_define_flags\n",
    "from train_mnist import train_model as scatternet_train\n",
    "from res_features import train as resnet_train\n",
    "from visualize import visualize_features\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "x_test, y_test = mnist.test.next_batch(1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Network & Logistic Regression\n",
    "The score function we use for comparison is just the mean number of labels correctly assigned to the test samples. The _viz features and labels used below were never used for training. The _viz features and labels were only used for TSNE visualizations and that is why we can still justify using them as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train_features, train_labels, test_features, test_labels):\n",
    "    lm = sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "    lm.fit(train_features, train_labels)\n",
    "    score = lm.score(test_features, test_labels)\n",
    "    return score\n",
    "\n",
    "def NN_score(nn, x_test, test_labels):\n",
    "    '''\n",
    "    nn here either represents the resnet or hybrid scatter network. Here we call them to generate a mean accuracy\n",
    "    \n",
    "    we don't submit the same test features here because the network is deterministic and will generate the same\n",
    "    intermediate features during the forward pass\n",
    "    '''\n",
    "    return nn.score(x_test, test_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION WITH RESIDUAL NETWORK AND FINE TUNING\n",
    "## Resnet 50 pretrained from the Kth layer and higher\n",
    "Below we generate two different pretrained residual networks. In both cases we generate a 50 layer residual network. The network variables are initialized with the values from a pretrained imagenet model. The difference between these two models is that one freezes weights before layer k while the other network leaves all variables as trainable. The inspiration behind this type of pretraining is because the initial layers of a CNN are learning features which are generally task invariant. As a result of this we make the assumption that the layers before K are \"good enough\" and our training should be spent on tuning the layers >= K.\n",
    "\n",
    "In the case of both resnets we stack an additional 3 fully connected layers on top of the flattened previously generated features. The first 2 layers use recified linear units as activation functions followed by a final layer which uses a softmax. All 3 of these layers initialize weights with a uniform Xavier initializer. More information on the motivation behind Xavier initializers can be found [here](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "The purpose of the final 3 stacked fully connected layers is to generate logits for classification. These layers can be thought of as replacements to the final fully connected layers generating logits for the 1000 imagenet classes. In addition to a classification mismatch between imagenet and MNIST we do not take variables from these layers because they are far more task dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/feature_viz/resnet/resnet_v1_50.ckpt\n",
      "iter 0 train accuracy: 0.09375\n",
      "iter 100 test accuracy: 0.6499999761581421\n",
      "EPOCH:  0\n",
      "saving after 100 iterations\n",
      "iter 100 train accuracy: 0.734375\n",
      "iter 200 test accuracy: 0.9200000166893005\n",
      "EPOCH:  0\n",
      "saving after 200 iterations\n",
      "iter 200 train accuracy: 0.8359375\n",
      "iter 300 test accuracy: 0.8799999952316284\n",
      "EPOCH:  0\n",
      "saving after 300 iterations\n",
      "iter 300 train accuracy: 0.9140625\n",
      "iter 400 test accuracy: 0.8799999952316284\n",
      "EPOCH:  0\n",
      "saving after 400 iterations\n",
      "iter 400 train accuracy: 0.890625\n",
      "iter 500 test accuracy: 0.9399999976158142\n",
      "EPOCH:  1\n",
      "saving after 500 iterations\n",
      "iter 500 train accuracy: 0.890625\n",
      "iter 600 test accuracy: 0.9100000262260437\n",
      "EPOCH:  1\n",
      "saving after 600 iterations\n",
      "iter 600 train accuracy: 0.953125\n",
      "iter 700 test accuracy: 0.9700000286102295\n",
      "EPOCH:  1\n",
      "saving after 700 iterations\n",
      "iter 700 train accuracy: 0.9296875\n",
      "iter 800 test accuracy: 0.9399999976158142\n",
      "EPOCH:  1\n",
      "saving after 800 iterations\n",
      "iter 800 train accuracy: 0.9296875\n",
      "fully connected neural network stacked on frozen resnet features accuracy:  0.93125\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/feature_viz/resnet/resnet_v1_50.ckpt\n",
      "iter 0 train accuracy: 0.09375\n",
      "iter 100 test accuracy: 0.9800000190734863\n",
      "EPOCH:  0\n",
      "saving after 100 iterations\n",
      "iter 100 train accuracy: 0.9140625\n",
      "iter 200 test accuracy: 0.9399999976158142\n",
      "EPOCH:  0\n",
      "saving after 200 iterations\n",
      "iter 200 train accuracy: 0.9296875\n",
      "iter 300 test accuracy: 0.9800000190734863\n",
      "EPOCH:  0\n",
      "saving after 300 iterations\n",
      "iter 300 train accuracy: 0.9609375\n",
      "iter 400 test accuracy: 0.9700000286102295\n",
      "EPOCH:  0\n",
      "saving after 400 iterations\n",
      "iter 400 train accuracy: 0.9609375\n",
      "iter 500 test accuracy: 0.9800000190734863\n",
      "EPOCH:  1\n",
      "saving after 500 iterations\n",
      "iter 500 train accuracy: 0.9609375\n",
      "iter 600 test accuracy: 0.9599999785423279\n",
      "EPOCH:  1\n",
      "saving after 600 iterations\n",
      "iter 600 train accuracy: 0.9765625\n",
      "iter 700 test accuracy: 0.9900000095367432\n",
      "EPOCH:  1\n",
      "saving after 700 iterations\n",
      "iter 700 train accuracy: 1.0\n",
      "iter 800 test accuracy: 0.9599999785423279\n",
      "EPOCH:  1\n",
      "saving after 800 iterations\n",
      "iter 800 train accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "resnet_classifier_k = resnet_train(freeze_before_k=3) # all blocks before block k are frozen for training\n",
    "resnet_k_features_test = resnet_classifier_k.get_features(x_test)\n",
    "\n",
    "nn_res_k_score = NN_score(resnet_classifier_k, x_test, y_test)\n",
    "print('fully connected neural network stacked on frozen resnet features accuracy: ', nn_res_k_score)\n",
    "\n",
    "tf.reset_default_graph() # clear graph for classifier_0\n",
    "resnet_classifier_0 = resnet_train(freeze_before_k=0)\n",
    "resnet_0_features_test = resnet_classifier_0.get_features(x_test)\n",
    "\n",
    "nn_res_0_score = NN_score(resnet_classifier_0, x_test, y_test)\n",
    "print('fully connected neural network stacked on unfrozen resnet features accuracy: ', nn_res_0_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION WITH SCATTER NETWORK\n",
    "## Scatternet Training and Testing on MNIST\n",
    "The work for scatternet was mostly based off the work done by [tdeboissiere](https://github.com/tdeboissiere) found [here](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/ScatteringTransform). This scatter network is actually a bit different than the scatter network found in Brunna & Mallat's work. What we analyze here is  a deep hybrid scatter network. We use scatter transforms similar to those found in [Brunna & Mallat's work](https://arxiv.org/abs/1203.1513) followed by a few convolutional layers and fully connected layers. A detailed explanation of how this works can be found in Oyallon's [Deep Hybrid Networks paper](https://arxiv.org/abs/1703.08961)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatternet_define_flags() \n",
    "scatternet_classifier = scatternet_train()  # no pretrained weights for the hybrid scatter net so we do not need to specify a k\n",
    "\n",
    "nn_scatternet_score = NN_score(scatternet_classifier, x_test, y_test)\n",
    "print('fully connected neural network stacked on scatter network features accuracy: ', nn_scatternet_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE VISUALIZATION\n",
    "Sample some data points from MNIST to map to a feature space with both of our classifiers. Once we have both of these new feature spaces we can use TSNE to reduce dimensionality to a space easily visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE on 50 layer residual network features\n",
    "Looks like we are forming some clusters of each of the classes however there is heavy overlap. Multiple clusters are being formed for some of the classes. Some interesting future work could be a more in depth cluster analysis. There looks to be 3 different clusters for images representing 9s. Perhaps one cluster contains 9s that are straight, another of 9s slanted to the left, and a final cluster of 9s slanted to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); resnet_classifier_k.load() # have to save and load b/c of conflicting params between resnets\n",
    "\n",
    "resnet_k_features_test = resnet_classifier_k.get_features(x_test)\n",
    "visualize_features(resnet_k_features_test, y_test, 'resnet50_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE on unfrozen 50 layer residual network features\n",
    "It appears that we generate better separation between classes with the unfrozen residual network. I suspect that this is the case because the MNIST dataset did not impose a data constraint on us. However, if we were to significantly reduce access to data I suspect that the frozen residual network would provide better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); resnet_classifier_0.load() # have to save and load b/c of conflicting params between resnets\\\n",
    "\n",
    "resnet_0_features_test = resnet_classifier_0.get_features(x_test)\n",
    "visualize_features(resnet_0_features_test, y_test, 'resnet50_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE on Deep Hybrid Network\n",
    "Class separation is much stronger here than we experienced in our 50 layer residual network. I am guessing this is because the Deep Hybrid Network allowed for all the weights to be trained. While we froze all the weights and biases below the 3rd block of the resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatternet_features_test = scatternet_classifier.get_features(x_test)\n",
    "visualize_features(scatternet_features_test, y_test, 'scatternet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE CLASSIFICATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train images and feature generation\n",
    "collect a batch of training images we can use as a training set for the logistic regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = mnist.train.next_batch(1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Hybrid Scatter Network: logistic regression vs fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatternet_features_train = scatternet_classifier.get_features(x_train)\n",
    "\n",
    "lm_scatternet_score = logistic_regression(scatternet_features_train, y_train, scatternet_features_test, y_test)\n",
    "print('logistic regression on deep hybrid scatter network features accuracy: ', lm_scatternet_score)\n",
    "\n",
    "nn_scatternet_score = NN_score(scatternet_classifier, x_test, y_test)\n",
    "print('fully connected neural network stacked on scatter network features accuracy: ', nn_scatternet_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen block K resnet: logistic regression vs fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); resnet_classifier_k.load() # have to save and load b/c of conflicting params between resnets\n",
    "resnet_k_features_train = resnet_classifier_k.get_features(x_train)\n",
    "\n",
    "lm_res_k_score = logistic_regression(resnet_k_features_train, y_train, resnet_k_features_test, y_test)\n",
    "print('logistic regression on frozen resnet features accuracy: ', lm_res_k_score)\n",
    "\n",
    "nn_res_k_score = NN_score(resnet_classifier_k, x_test, y_test)\n",
    "print('fully connected neural network stacked on frozen resnet features accuracy: ', nn_res_k_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfrozen resnet: logistic regression vs fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); resnet_classifier_0.load() # have to save and load b/c of conflicting params between resnets\n",
    "resnet_0_features_train = resnet_classifier_0.get_features(x_train)\n",
    "\n",
    "lm_res_0_score = logistic_regression(resnet_0_features_train, y_train, resnet_0_features_test, y_test)\n",
    "print('logistic regression on unfrozen resnet features accuracy: ', lm_res_0_score)\n",
    "\n",
    "nn_res_0_score = NN_score(resnet_classifier_0, x_test, y_test)\n",
    "print('fully connected neural network stacked on unfrozen resnet features accuracy: ', nn_res_0_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "In our analysis we compared 6 different types of classifiers. The 6 classifiers fell into a few different sub groups. First, the features were either generated from a 50 layer residual network or a deep hybrid network. Second The variables included in the 50 layer residual network were either frozen if they belonged to a layer earlier than K or they were initialized but left free to train. Third, the actual classification was done in one of two ways. The first choice of classification was using a basic multiclass logistic regression model. Our logistic regression model used saga for optimization. More information about saga can be read [here](https://www.di.ens.fr/~fbach/Defazio_NIPS2014.pdf) in a 2014 NIPs paper. The second choice of classification was using a multilayer fully connected neural network.\n",
    "\n",
    "1. The Unfrozen residual network and the deep hybrid network performed very similarly. It is unclear which of these two is actually supperior at this task. The Deep Hybrid Network performed marginally better, but hyperparams and preprocessing were optized for MNIST. While in the case of the residual network there was no preprocessing techniques used aside from resizing the imge. Hyperparameters were also left as default for the residual network. I am also unsure if any image preprocessing techniques were used when training the initialized residual network on imagenet. For example we did not apply mean shifting to mnist images and if mean shifting each channel was used during pretraining this could render the initialized weights useless.\n",
    "\n",
    "1. an interesting next piece of work would be to make a comparison between the features generated by a basic Scattering Convolutional Network with a Deep Hybrid Network.\n",
    "\n",
    "| Frozen Resnet LR | Frozen Resnet NN | Unfrozen Resnet LR | Unfrozen Resnet NN | Deep Hybrid Network LR | Deep Hybrid Network NN\n",
    "------------ | ------------- | ------------- | ------------- | ------------- | ------------- |\n",
    "test | 0.9 | 0.8 | 0.7 | 0.6 | 0.5 | 0.4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
