{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports of utility functions we will be using\n",
    "below we import the work for our residual network and deep hybrid scatter network. All the real 'heavy lifting' for this project was done in the resnet and ScatterTransform scripts. For a more detailed look at what's going on under the hood go give the scripts a peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/ubuntu/feature_viz/resnet')\n",
    "sys.path.append('/home/ubuntu/feature_viz/ScatteringTransform/src/model')\n",
    "from flags import define_flags as scatternet_define_flags\n",
    "from train_mnist import train_model as scatternet_train\n",
    "from res_features import train as resnet_train\n",
    "from visualize import visualize_features\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION WITH RESIDUAL NETWORK AND FINE TUNING\n",
    "## Resnet 50 pretrained from the Kth layer and higher\n",
    "Below we generate two different pretrained residual networks. In both cases we generate a 50 layer residual network. The network variables are initialized with the values from a pretrained imagenet model. The difference between these two models is that one freezes weights before layer k while the other network leaves all variables as trainable. The inspiration behind this type of pretraining is because the initial layers of a CNN are learning features which are generally task invariant. As a result of this we make the assumption that the layers before K are \"good enough\" and our training should be spent on tuning the layers >= K.\n",
    "\n",
    "In the case of both resnets we stack an additional 3 fully connected layers on top of the flattened previously generated features. The first 2 layers use recified linear units as activation functions followed by a final layer which uses a softmax. All 3 of these layers initialize weights with a uniform Xavier initializer. More information on the motivation behind Xavier initializers can be found [here](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    "The purpose of the final 3 stacked fully connected layers is to generate logits for classification. These layers can be thought of as replacements to the final fully connected layers generating logits for the 1000 imagenet classes. In addition to a classification mismatch between imagenet and MNIST we do not take variables from these layers because they are far more task dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/feature_viz/resnet/resnet_v1_50.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/feature_viz/resnet/mnist_ckpt_block3/mnist-800\n",
      "iter 800 train accuracy: 0.9140625\n",
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/feature_viz/resnet/resnet_v1_50.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/feature_viz/resnet/mnist_ckpt_block0/mnist-800\n",
      "iter 800 train accuracy: 0.96875\n"
     ]
    }
   ],
   "source": [
    "resnet_classifier_k = resnet_train(freeze_before_k=3) # all blocks before block k are frozen for training\n",
    "tf.reset_default_graph() # clear graph for classifier_0\n",
    "resnet_classifier_0 = resnet_train(freeze_before_k=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION WITH SCATTER NETWORK\n",
    "## Scatternet Training and Testing on MNIST\n",
    "The work for scatternet was mostly based off the work done by [tdeboissiere](https://github.com/tdeboissiere) found [here](https://github.com/tdeboissiere/DeepLearningImplementations/tree/master/ScatteringTransform). This scatter network is actually a bit different than the scatter network found in Brunna & Mallat's work. What we analyze here is  a deep hybrid scatter network. We use scatter transforms similar to those found in [Brunna & Mallat's work](https://arxiv.org/abs/1203.1513) followed by a few convolutional layers and fully connected layers. A detailed explanation of how this works can be found in Oyallon's [Deep Hybrid Networks paper](https://arxiv.org/abs/1703.08961)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setting up TF session:\n",
      "\n",
      "Configuring directories:\n",
      "[Deleting] /home/ubuntu/feature_viz/ScatteringTransform/src/logs\n",
      "[Deleting] /home/ubuntu/feature_viz/ScatteringTransform/src/models\n",
      "[Deleting] /home/ubuntu/feature_viz/ScatteringTransform/src/figures\n",
      "[Creating] /home/ubuntu/feature_viz/ScatteringTransform/src/logs\n",
      "[Creating] /home/ubuntu/feature_viz/ScatteringTransform/src/models\n",
      "[Creating] /home/ubuntu/feature_viz/ScatteringTransform/src/figures\n",
      "Extracting /home/ubuntu/feature_viz/ScatteringTransform/src/data/raw/train-images-idx3-ubyte.gz\n",
      "Extracting /home/ubuntu/feature_viz/ScatteringTransform/src/data/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/ubuntu/feature_viz/ScatteringTransform/src/data/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/ubuntu/feature_viz/ScatteringTransform/src/data/raw/t10k-labels-idx1-ubyte.gz\n",
      "INFO:tensorflow:Summary name HCNN/CONV2D/conv2d/w:0 is illegal; using HCNN/CONV2D/conv2d/w_0 instead.\n",
      "INFO:tensorflow:Summary name HCNN/CONV2D/conv2d/b:0 is illegal; using HCNN/CONV2D/conv2d/b_0 instead.\n",
      "INFO:tensorflow:Summary name HCNN/dense1/w:0 is illegal; using HCNN/dense1/w_0 instead.\n",
      "INFO:tensorflow:Summary name HCNN/dense1/b:0 is illegal; using HCNN/dense1/b_0 instead.\n",
      "INFO:tensorflow:Summary name HCNN/dense2/w:0 is illegal; using HCNN/dense2/w_0 instead.\n",
      "INFO:tensorflow:Summary name HCNN/dense2/b:0 is illegal; using HCNN/dense2/b_0 instead.\n",
      "INFO:tensorflow:Summary name HCNN/scat_bn/beta:0/gradient is illegal; using HCNN/scat_bn/beta_0/gradient instead.\n",
      "INFO:tensorflow:Summary name HCNN/CONV2D/conv2d/w:0/gradient is illegal; using HCNN/CONV2D/conv2d/w_0/gradient instead.\n",
      "INFO:tensorflow:Summary name HCNN/CONV2D/conv2d/b:0/gradient is illegal; using HCNN/CONV2D/conv2d/b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name HCNN/dense1/w:0/gradient is illegal; using HCNN/dense1/w_0/gradient instead.\n",
      "INFO:tensorflow:Summary name HCNN/dense1/b:0/gradient is illegal; using HCNN/dense1/b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name HCNN/dense2/w:0/gradient is illegal; using HCNN/dense2/w_0/gradient instead.\n",
      "INFO:tensorflow:Summary name HCNN/dense2/b:0/gradient is illegal; using HCNN/dense2/b_0/gradient instead.\n",
      "\n",
      "Initialization:\n",
      "[Created session saver] \n",
      "[Ran init ops] \n",
      "\n",
      "Queues:\n",
      "[Created coordinator] \n",
      "[Started queue runner] \n",
      "\n",
      "Summaries:\n",
      "[HCNN/CONV2D/conv2d/HCNN/CONV2D/conv2d/w_0:0] \n",
      "[HCNN/CONV2D/conv2d/HCNN/CONV2D/conv2d/b_0:0] \n",
      "[HCNN/dense1/HCNN/dense1/w_0:0] \n",
      "[HCNN/dense1/HCNN/dense1/b_0:0] \n",
      "[HCNN/dense2/HCNN/dense2/w_0:0] \n",
      "[HCNN/dense2/HCNN/dense2/b_0:0] \n",
      "[HCNN/scat_bn/beta_0/gradient:0] \n",
      "[HCNN/CONV2D/conv2d/w_0/gradient:0] \n",
      "[HCNN/CONV2D/conv2d/b_0/gradient:0] \n",
      "[HCNN/dense1/w_0/gradient:0] \n",
      "[HCNN/dense1/b_0/gradient:0] \n",
      "[HCNN/dense2/w_0/gradient:0] \n",
      "[HCNN/dense2/b_0/gradient:0] \n",
      "[loss:0] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training progress:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 0: - train loss: 2.35 val loss: 2.47 - train acc: 0.10 val acc: 0.09:   0%|          | 0/30 [00:03<?, ?it/s]\n",
      "Epoch 0: - train loss: 2.35 val loss: 2.47 - train acc: 0.10 val acc: 0.09:   3%|▎         | 1/30 [00:03<01:40,  3.46s/it]\n",
      "Epoch 0: - train loss: 2.38 val loss: 2.39 - train acc: 0.05 val acc: 0.12:   3%|▎         | 1/30 [00:03<01:49,  3.76s/it]\n",
      "Epoch 0: - train loss: 2.40 val loss: 2.36 - train acc: 0.12 val acc: 0.12:   3%|▎         | 1/30 [00:04<01:57,  4.07s/it]\n",
      "Epoch 0: - train loss: 2.40 val loss: 2.36 - train acc: 0.12 val acc: 0.12:  10%|█         | 3/30 [00:04<00:36,  1.36s/it]\n",
      "Epoch 0: - train loss: 2.33 val loss: 2.37 - train acc: 0.15 val acc: 0.13:  10%|█         | 3/30 [00:04<00:39,  1.46s/it]\n",
      "Epoch 0: - train loss: 2.36 val loss: 2.27 - train acc: 0.12 val acc: 0.14:  10%|█         | 3/30 [00:04<00:41,  1.56s/it]\n",
      "Epoch 0: - train loss: 2.36 val loss: 2.27 - train acc: 0.12 val acc: 0.14:  17%|█▋        | 5/30 [00:04<00:23,  1.07it/s]\n",
      "Epoch 0: - train loss: 2.35 val loss: 2.37 - train acc: 0.11 val acc: 0.13:  17%|█▋        | 5/30 [00:04<00:24,  1.01it/s]\n",
      "Epoch 0: - train loss: 2.29 val loss: 2.32 - train acc: 0.16 val acc: 0.17:  17%|█▋        | 5/30 [00:05<00:26,  1.05s/it]\n",
      "Epoch 0: - train loss: 2.29 val loss: 2.32 - train acc: 0.16 val acc: 0.17:  23%|██▎       | 7/30 [00:05<00:17,  1.33it/s]\n",
      "Epoch 0: - train loss: 2.28 val loss: 2.30 - train acc: 0.16 val acc: 0.17:  23%|██▎       | 7/30 [00:05<00:18,  1.26it/s]\n",
      "Epoch 0: - train loss: 2.26 val loss: 2.33 - train acc: 0.19 val acc: 0.18:  23%|██▎       | 7/30 [00:05<00:19,  1.20it/s]\n",
      "Epoch 0: - train loss: 2.26 val loss: 2.33 - train acc: 0.19 val acc: 0.18:  30%|███       | 9/30 [00:05<00:13,  1.54it/s]\n",
      "Epoch 0: - train loss: 2.36 val loss: 2.26 - train acc: 0.12 val acc: 0.18:  30%|███       | 9/30 [00:06<00:14,  1.46it/s]\n",
      "Epoch 0: - train loss: 2.42 val loss: 2.27 - train acc: 0.10 val acc: 0.20:  30%|███       | 9/30 [00:06<00:15,  1.39it/s]\n",
      "Epoch 0: - train loss: 2.42 val loss: 2.27 - train acc: 0.10 val acc: 0.20:  37%|███▋      | 11/30 [00:06<00:11,  1.70it/s]\n",
      "Epoch 0: - train loss: 2.28 val loss: 2.29 - train acc: 0.16 val acc: 0.16:  37%|███▋      | 11/30 [00:06<00:11,  1.63it/s]\n",
      "Epoch 0: - train loss: 2.24 val loss: 2.31 - train acc: 0.17 val acc: 0.14:  37%|███▋      | 11/30 [00:07<00:12,  1.56it/s]\n",
      "Epoch 0: - train loss: 2.24 val loss: 2.31 - train acc: 0.17 val acc: 0.14:  43%|████▎     | 13/30 [00:07<00:09,  1.84it/s]\n",
      "Epoch 0: - train loss: 2.23 val loss: 2.27 - train acc: 0.20 val acc: 0.22:  43%|████▎     | 13/30 [00:07<00:09,  1.77it/s]\n",
      "Epoch 0: - train loss: 2.25 val loss: 2.27 - train acc: 0.20 val acc: 0.18:  43%|████▎     | 13/30 [00:07<00:10,  1.70it/s]\n",
      "Epoch 0: - train loss: 2.25 val loss: 2.27 - train acc: 0.20 val acc: 0.18:  50%|█████     | 15/30 [00:07<00:07,  1.96it/s]\n",
      "Epoch 0: - train loss: 2.21 val loss: 2.25 - train acc: 0.24 val acc: 0.21:  50%|█████     | 15/30 [00:07<00:07,  1.89it/s]\n",
      "Epoch 0: - train loss: 2.28 val loss: 2.19 - train acc: 0.21 val acc: 0.23:  50%|█████     | 15/30 [00:08<00:08,  1.82it/s]\n",
      "Epoch 0: - train loss: 2.28 val loss: 2.19 - train acc: 0.21 val acc: 0.23:  57%|█████▋    | 17/30 [00:08<00:06,  2.06it/s]\n",
      "Epoch 0: - train loss: 2.15 val loss: 2.20 - train acc: 0.28 val acc: 0.23:  57%|█████▋    | 17/30 [00:08<00:06,  1.98it/s]\n",
      "Epoch 0: - train loss: 2.24 val loss: 2.24 - train acc: 0.16 val acc: 0.21:  57%|█████▋    | 17/30 [00:08<00:06,  1.91it/s]\n",
      "Epoch 0: - train loss: 2.24 val loss: 2.24 - train acc: 0.16 val acc: 0.21:  63%|██████▎   | 19/30 [00:08<00:05,  2.14it/s]\n",
      "Epoch 0: - train loss: 2.23 val loss: 2.20 - train acc: 0.19 val acc: 0.25:  63%|██████▎   | 19/30 [00:09<00:05,  2.06it/s]\n",
      "Epoch 0: - train loss: 2.17 val loss: 2.18 - train acc: 0.27 val acc: 0.30:  63%|██████▎   | 19/30 [00:09<00:05,  2.00it/s]\n",
      "Epoch 0: - train loss: 2.17 val loss: 2.18 - train acc: 0.27 val acc: 0.30:  70%|███████   | 21/30 [00:09<00:04,  2.21it/s]\n",
      "Epoch 0: - train loss: 2.22 val loss: 2.25 - train acc: 0.20 val acc: 0.21:  70%|███████   | 21/30 [00:09<00:04,  2.14it/s]\n",
      "Epoch 0: - train loss: 2.29 val loss: 2.19 - train acc: 0.22 val acc: 0.29:  70%|███████   | 21/30 [00:10<00:04,  2.08it/s]\n",
      "Epoch 0: - train loss: 2.29 val loss: 2.19 - train acc: 0.22 val acc: 0.29:  77%|███████▋  | 23/30 [00:10<00:03,  2.27it/s]\n",
      "Epoch 0: - train loss: 2.17 val loss: 2.22 - train acc: 0.27 val acc: 0.23:  77%|███████▋  | 23/30 [00:10<00:03,  2.21it/s]\n",
      "Epoch 0: - train loss: 2.13 val loss: 2.20 - train acc: 0.29 val acc: 0.29:  77%|███████▋  | 23/30 [00:10<00:03,  2.14it/s]\n",
      "Epoch 0: - train loss: 2.13 val loss: 2.20 - train acc: 0.29 val acc: 0.29:  83%|████████▎ | 25/30 [00:10<00:02,  2.33it/s]\n",
      "Epoch 0: - train loss: 2.23 val loss: 2.10 - train acc: 0.20 val acc: 0.28:  83%|████████▎ | 25/30 [00:11<00:02,  2.27it/s]\n",
      "Epoch 0: - train loss: 2.11 val loss: 2.15 - train acc: 0.30 val acc: 0.27:  83%|████████▎ | 25/30 [00:11<00:02,  2.21it/s]\n",
      "Epoch 0: - train loss: 2.11 val loss: 2.15 - train acc: 0.30 val acc: 0.27:  90%|█████████ | 27/30 [00:11<00:01,  2.38it/s]\n",
      "Epoch 0: - train loss: 2.07 val loss: 2.15 - train acc: 0.40 val acc: 0.28:  90%|█████████ | 27/30 [00:11<00:01,  2.32it/s]\n",
      "Epoch 0: - train loss: 2.17 val loss: 2.06 - train acc: 0.35 val acc: 0.35:  90%|█████████ | 27/30 [00:11<00:01,  2.27it/s]\n",
      "Epoch 0: - train loss: 2.17 val loss: 2.06 - train acc: 0.35 val acc: 0.35:  97%|█████████▋| 29/30 [00:11<00:00,  2.43it/s]\n",
      "Epoch 0: - train loss: 2.19 val loss: 2.11 - train acc: 0.23 val acc: 0.34:  97%|█████████▋| 29/30 [00:12<00:00,  2.37it/s]\n",
      "Training progress:  10%|█         | 1/10 [00:12<01:50, 12.22s/it]acc: 0.34: 100%|██████████| 30/30 [00:12<00:00,  2.46it/s]\n",
      "Epoch 1:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 1: - train loss: 2.14 val loss: 2.15 - train acc: 0.33 val acc: 0.30:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 1: - train loss: 2.07 val loss: 2.16 - train acc: 0.34 val acc: 0.29:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 1: - train loss: 2.07 val loss: 2.16 - train acc: 0.34 val acc: 0.29:   7%|▋         | 2/30 [00:00<00:08,  3.27it/s]\n",
      "Epoch 1: - train loss: 2.09 val loss: 2.11 - train acc: 0.34 val acc: 0.28:   7%|▋         | 2/30 [00:00<00:12,  2.22it/s]\n",
      "Epoch 1: - train loss: 2.00 val loss: 2.10 - train acc: 0.38 val acc: 0.32:   7%|▋         | 2/30 [00:01<00:16,  1.67it/s]\n",
      "Epoch 1: - train loss: 2.00 val loss: 2.10 - train acc: 0.38 val acc: 0.32:  13%|█▎        | 4/30 [00:01<00:07,  3.34it/s]\n",
      "Epoch 1: - train loss: 2.06 val loss: 2.12 - train acc: 0.41 val acc: 0.26:  13%|█▎        | 4/30 [00:01<00:09,  2.69it/s]\n",
      "Epoch 1: - train loss: 2.03 val loss: 2.03 - train acc: 0.42 val acc: 0.35:  13%|█▎        | 4/30 [00:01<00:11,  2.25it/s]\n",
      "Epoch 1: - train loss: 2.03 val loss: 2.03 - train acc: 0.42 val acc: 0.35:  20%|██        | 6/30 [00:01<00:07,  3.37it/s]\n",
      "Epoch 1: - train loss: 2.09 val loss: 2.14 - train acc: 0.30 val acc: 0.30:  20%|██        | 6/30 [00:02<00:08,  2.86it/s]\n",
      "Epoch 1: - train loss: 2.06 val loss: 2.05 - train acc: 0.35 val acc: 0.34:  20%|██        | 6/30 [00:02<00:09,  2.48it/s]\n",
      "Epoch 1: - train loss: 2.06 val loss: 2.05 - train acc: 0.35 val acc: 0.34:  27%|██▋       | 8/30 [00:02<00:06,  3.31it/s]\n",
      "Epoch 1: - train loss: 2.04 val loss: 2.00 - train acc: 0.38 val acc: 0.41:  27%|██▋       | 8/30 [00:02<00:07,  2.95it/s]\n",
      "Epoch 1: - train loss: 1.96 val loss: 2.02 - train acc: 0.45 val acc: 0.37:  27%|██▋       | 8/30 [00:03<00:08,  2.66it/s]\n",
      "Epoch 1: - train loss: 1.96 val loss: 2.02 - train acc: 0.45 val acc: 0.37:  33%|███▎      | 10/30 [00:03<00:06,  3.33it/s]\n",
      "Epoch 1: - train loss: 2.05 val loss: 2.06 - train acc: 0.34 val acc: 0.34:  33%|███▎      | 10/30 [00:03<00:06,  3.04it/s]\n",
      "Epoch 1: - train loss: 2.01 val loss: 2.07 - train acc: 0.33 val acc: 0.30:  33%|███▎      | 10/30 [00:03<00:07,  2.79it/s]\n",
      "Epoch 1: - train loss: 2.01 val loss: 2.07 - train acc: 0.33 val acc: 0.30:  40%|████      | 12/30 [00:03<00:05,  3.34it/s]\n",
      "Epoch 1: - train loss: 2.03 val loss: 2.05 - train acc: 0.40 val acc: 0.36:  40%|████      | 12/30 [00:03<00:05,  3.09it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: - train loss: 2.02 val loss: 2.03 - train acc: 0.40 val acc: 0.38:  40%|████      | 12/30 [00:04<00:06,  2.86it/s]\n",
      "Epoch 1: - train loss: 2.02 val loss: 2.03 - train acc: 0.40 val acc: 0.38:  47%|████▋     | 14/30 [00:04<00:04,  3.34it/s]\n",
      "Epoch 1: - train loss: 1.99 val loss: 1.97 - train acc: 0.40 val acc: 0.45:  47%|████▋     | 14/30 [00:04<00:05,  3.12it/s]\n",
      "Epoch 1: - train loss: 1.99 val loss: 1.97 - train acc: 0.38 val acc: 0.37:  47%|████▋     | 14/30 [00:04<00:05,  2.93it/s]\n",
      "Epoch 1: - train loss: 1.99 val loss: 1.97 - train acc: 0.38 val acc: 0.37:  53%|█████▎    | 16/30 [00:04<00:04,  3.35it/s]\n",
      "Epoch 1: - train loss: 2.00 val loss: 2.01 - train acc: 0.38 val acc: 0.36:  53%|█████▎    | 16/30 [00:05<00:04,  3.16it/s]\n",
      "Epoch 1: - train loss: 1.98 val loss: 1.93 - train acc: 0.45 val acc: 0.48:  53%|█████▎    | 16/30 [00:05<00:04,  2.98it/s]\n",
      "Epoch 1: - train loss: 1.98 val loss: 1.93 - train acc: 0.45 val acc: 0.48:  60%|██████    | 18/30 [00:05<00:03,  3.35it/s]\n",
      "Epoch 1: - train loss: 1.95 val loss: 1.98 - train acc: 0.46 val acc: 0.39:  60%|██████    | 18/30 [00:05<00:03,  3.18it/s]\n",
      "Epoch 1: - train loss: 1.92 val loss: 1.96 - train acc: 0.49 val acc: 0.44:  60%|██████    | 18/30 [00:05<00:03,  3.03it/s]\n",
      "Epoch 1: - train loss: 1.92 val loss: 1.96 - train acc: 0.49 val acc: 0.44:  67%|██████▋   | 20/30 [00:05<00:02,  3.36it/s]\n",
      "Epoch 1: - train loss: 1.92 val loss: 1.93 - train acc: 0.45 val acc: 0.46:  67%|██████▋   | 20/30 [00:06<00:03,  3.20it/s]\n",
      "Epoch 1: - train loss: 1.86 val loss: 1.99 - train acc: 0.55 val acc: 0.33:  67%|██████▋   | 20/30 [00:06<00:03,  3.06it/s]\n",
      "Epoch 1: - train loss: 1.86 val loss: 1.99 - train acc: 0.55 val acc: 0.33:  73%|███████▎  | 22/30 [00:06<00:02,  3.37it/s]\n",
      "Epoch 1: - train loss: 1.93 val loss: 1.87 - train acc: 0.45 val acc: 0.54:  73%|███████▎  | 22/30 [00:06<00:02,  3.22it/s]\n",
      "Epoch 1: - train loss: 1.90 val loss: 2.01 - train acc: 0.50 val acc: 0.41:  73%|███████▎  | 22/30 [00:07<00:02,  3.06it/s]\n",
      "Epoch 1: - train loss: 1.90 val loss: 2.01 - train acc: 0.50 val acc: 0.41:  80%|████████  | 24/30 [00:07<00:01,  3.33it/s]\n",
      "Epoch 1: - train loss: 1.93 val loss: 1.93 - train acc: 0.47 val acc: 0.41:  80%|████████  | 24/30 [00:07<00:01,  3.18it/s]\n",
      "Epoch 1: - train loss: 1.89 val loss: 1.92 - train acc: 0.54 val acc: 0.47:  80%|████████  | 24/30 [00:07<00:01,  3.06it/s]\n",
      "Epoch 1: - train loss: 1.89 val loss: 1.92 - train acc: 0.54 val acc: 0.47:  87%|████████▋ | 26/30 [00:07<00:01,  3.31it/s]\n",
      "Epoch 1: - train loss: 1.92 val loss: 1.88 - train acc: 0.42 val acc: 0.48:  87%|████████▋ | 26/30 [00:08<00:01,  3.17it/s]\n",
      "Epoch 1: - train loss: 1.88 val loss: 1.88 - train acc: 0.50 val acc: 0.53:  87%|████████▋ | 26/30 [00:08<00:01,  3.06it/s]\n",
      "Epoch 1: - train loss: 1.88 val loss: 1.88 - train acc: 0.50 val acc: 0.53:  93%|█████████▎| 28/30 [00:08<00:00,  3.30it/s]\n",
      "Epoch 1: - train loss: 1.88 val loss: 1.90 - train acc: 0.49 val acc: 0.46:  93%|█████████▎| 28/30 [00:08<00:00,  3.19it/s]\n",
      "Epoch 1: - train loss: 1.89 val loss: 1.86 - train acc: 0.48 val acc: 0.48:  93%|█████████▎| 28/30 [00:09<00:00,  3.08it/s]\n",
      "Epoch 1: - train loss: 1.89 val loss: 1.86 - train acc: 0.48 val acc: 0.48: 100%|██████████| 30/30 [00:09<00:00,  3.30it/s]\n",
      "Training progress:  20%|██        | 2/10 [00:21<01:25, 10.67s/it]\n",
      "Epoch 2:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 2: - train loss: 1.84 val loss: 1.82 - train acc: 0.52 val acc: 0.55:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 2: - train loss: 1.83 val loss: 1.82 - train acc: 0.54 val acc: 0.53:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 2: - train loss: 1.83 val loss: 1.82 - train acc: 0.54 val acc: 0.53:   7%|▋         | 2/30 [00:00<00:08,  3.43it/s]\n",
      "Epoch 2: - train loss: 1.85 val loss: 1.84 - train acc: 0.50 val acc: 0.51:   7%|▋         | 2/30 [00:00<00:12,  2.29it/s]\n",
      "Epoch 2: - train loss: 1.79 val loss: 1.88 - train acc: 0.59 val acc: 0.53:   7%|▋         | 2/30 [00:01<00:16,  1.72it/s]\n",
      "Epoch 2: - train loss: 1.79 val loss: 1.88 - train acc: 0.59 val acc: 0.53:  13%|█▎        | 4/30 [00:01<00:07,  3.43it/s]\n",
      "Epoch 2: - train loss: 1.87 val loss: 1.87 - train acc: 0.54 val acc: 0.49:  13%|█▎        | 4/30 [00:01<00:09,  2.70it/s]\n",
      "Epoch 2: - train loss: 1.80 val loss: 1.82 - train acc: 0.55 val acc: 0.57:  13%|█▎        | 4/30 [00:01<00:11,  2.22it/s]\n",
      "Epoch 2: - train loss: 1.80 val loss: 1.82 - train acc: 0.55 val acc: 0.57:  20%|██        | 6/30 [00:01<00:07,  3.33it/s]\n",
      "Epoch 2: - train loss: 1.76 val loss: 1.88 - train acc: 0.55 val acc: 0.47:  20%|██        | 6/30 [00:02<00:08,  2.87it/s]\n",
      "Epoch 2: - train loss: 1.80 val loss: 1.80 - train acc: 0.55 val acc: 0.57:  20%|██        | 6/30 [00:02<00:09,  2.49it/s]\n",
      "Epoch 2: - train loss: 1.80 val loss: 1.80 - train acc: 0.55 val acc: 0.57:  27%|██▋       | 8/30 [00:02<00:06,  3.32it/s]\n",
      "Epoch 2: - train loss: 1.86 val loss: 1.79 - train acc: 0.49 val acc: 0.57:  27%|██▋       | 8/30 [00:02<00:07,  2.96it/s]\n",
      "Epoch 2: - train loss: 1.79 val loss: 1.80 - train acc: 0.54 val acc: 0.55:  27%|██▋       | 8/30 [00:03<00:08,  2.65it/s]\n",
      "Epoch 2: - train loss: 1.79 val loss: 1.80 - train acc: 0.54 val acc: 0.55:  33%|███▎      | 10/30 [00:03<00:06,  3.30it/s]\n",
      "Epoch 2: - train loss: 1.89 val loss: 1.85 - train acc: 0.48 val acc: 0.54:  33%|███▎      | 10/30 [00:03<00:06,  3.02it/s]\n",
      "Epoch 2: - train loss: 1.75 val loss: 1.81 - train acc: 0.57 val acc: 0.58:  33%|███▎      | 10/30 [00:03<00:07,  2.76it/s]\n",
      "Epoch 2: - train loss: 1.75 val loss: 1.81 - train acc: 0.57 val acc: 0.58:  40%|████      | 12/30 [00:03<00:05,  3.31it/s]\n",
      "Epoch 2: - train loss: 1.78 val loss: 1.78 - train acc: 0.50 val acc: 0.53:  40%|████      | 12/30 [00:03<00:05,  3.05it/s]\n",
      "Epoch 2: - train loss: 1.74 val loss: 1.72 - train acc: 0.59 val acc: 0.62:  40%|████      | 12/30 [00:04<00:06,  2.84it/s]\n",
      "Epoch 2: - train loss: 1.74 val loss: 1.72 - train acc: 0.59 val acc: 0.62:  47%|████▋     | 14/30 [00:04<00:04,  3.31it/s]\n",
      "Epoch 2: - train loss: 1.74 val loss: 1.82 - train acc: 0.56 val acc: 0.56:  47%|████▋     | 14/30 [00:04<00:05,  3.10it/s]\n",
      "Epoch 2: - train loss: 1.76 val loss: 1.73 - train acc: 0.59 val acc: 0.60:  47%|████▋     | 14/30 [00:04<00:05,  2.89it/s]\n",
      "Epoch 2: - train loss: 1.76 val loss: 1.73 - train acc: 0.59 val acc: 0.60:  53%|█████▎    | 16/30 [00:04<00:04,  3.30it/s]\n",
      "Epoch 2: - train loss: 1.72 val loss: 1.75 - train acc: 0.60 val acc: 0.55:  53%|█████▎    | 16/30 [00:05<00:04,  3.12it/s]\n",
      "Epoch 2: - train loss: 1.79 val loss: 1.79 - train acc: 0.58 val acc: 0.55:  53%|█████▎    | 16/30 [00:05<00:04,  2.95it/s]\n",
      "Epoch 2: - train loss: 1.79 val loss: 1.79 - train acc: 0.58 val acc: 0.55:  60%|██████    | 18/30 [00:05<00:03,  3.32it/s]\n",
      "Epoch 2: - train loss: 1.75 val loss: 1.67 - train acc: 0.57 val acc: 0.61:  60%|██████    | 18/30 [00:05<00:03,  3.15it/s]\n",
      "Epoch 2: - train loss: 1.70 val loss: 1.69 - train acc: 0.60 val acc: 0.65:  60%|██████    | 18/30 [00:06<00:04,  2.99it/s]\n",
      "Epoch 2: - train loss: 1.70 val loss: 1.69 - train acc: 0.60 val acc: 0.65:  67%|██████▋   | 20/30 [00:06<00:03,  3.33it/s]\n",
      "Epoch 2: - train loss: 1.72 val loss: 1.69 - train acc: 0.63 val acc: 0.65:  67%|██████▋   | 20/30 [00:06<00:03,  3.17it/s]\n",
      "Epoch 2: - train loss: 1.69 val loss: 1.65 - train acc: 0.63 val acc: 0.66:  67%|██████▋   | 20/30 [00:06<00:03,  3.03it/s]\n",
      "Epoch 2: - train loss: 1.69 val loss: 1.65 - train acc: 0.63 val acc: 0.66:  73%|███████▎  | 22/30 [00:06<00:02,  3.34it/s]\n",
      "Epoch 2: - train loss: 1.67 val loss: 1.76 - train acc: 0.65 val acc: 0.54:  73%|███████▎  | 22/30 [00:06<00:02,  3.20it/s]\n",
      "Epoch 2: - train loss: 1.70 val loss: 1.75 - train acc: 0.64 val acc: 0.62:  73%|███████▎  | 22/30 [00:07<00:02,  3.06it/s]\n",
      "Epoch 2: - train loss: 1.70 val loss: 1.75 - train acc: 0.64 val acc: 0.62:  80%|████████  | 24/30 [00:07<00:01,  3.34it/s]\n",
      "Epoch 2: - train loss: 1.64 val loss: 1.63 - train acc: 0.69 val acc: 0.70:  80%|████████  | 24/30 [00:07<00:01,  3.21it/s]\n",
      "Epoch 2: - train loss: 1.64 val loss: 1.66 - train acc: 0.67 val acc: 0.66:  80%|████████  | 24/30 [00:07<00:01,  3.09it/s]\n",
      "Epoch 2: - train loss: 1.64 val loss: 1.66 - train acc: 0.67 val acc: 0.66:  87%|████████▋ | 26/30 [00:07<00:01,  3.35it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: - train loss: 1.70 val loss: 1.57 - train acc: 0.61 val acc: 0.65:  87%|████████▋ | 26/30 [00:08<00:01,  3.23it/s]\n",
      "Epoch 2: - train loss: 1.71 val loss: 1.74 - train acc: 0.57 val acc: 0.62:  87%|████████▋ | 26/30 [00:08<00:01,  3.11it/s]\n",
      "Epoch 2: - train loss: 1.71 val loss: 1.74 - train acc: 0.57 val acc: 0.62:  93%|█████████▎| 28/30 [00:08<00:00,  3.35it/s]\n",
      "Epoch 2: - train loss: 1.64 val loss: 1.66 - train acc: 0.69 val acc: 0.68:  93%|█████████▎| 28/30 [00:08<00:00,  3.24it/s]\n",
      "Epoch 2: - train loss: 1.65 val loss: 1.64 - train acc: 0.64 val acc: 0.69:  93%|█████████▎| 28/30 [00:08<00:00,  3.13it/s]\n",
      "Epoch 2: - train loss: 1.65 val loss: 1.64 - train acc: 0.64 val acc: 0.69: 100%|██████████| 30/30 [00:08<00:00,  3.35it/s]\n",
      "Training progress:  30%|███       | 3/10 [00:30<01:10, 10.10s/it]\n",
      "Epoch 3:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 3: - train loss: 1.61 val loss: 1.62 - train acc: 0.69 val acc: 0.64:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 3: - train loss: 1.58 val loss: 1.58 - train acc: 0.66 val acc: 0.67:   0%|          | 0/30 [00:00<?, ?it/s]\n",
      "Epoch 3: - train loss: 1.58 val loss: 1.58 - train acc: 0.66 val acc: 0.67:   7%|▋         | 2/30 [00:00<00:08,  3.42it/s]\n",
      "Epoch 3: - train loss: 1.68 val loss: 1.60 - train acc: 0.61 val acc: 0.68:   7%|▋         | 2/30 [00:00<00:12,  2.28it/s]\n",
      "Epoch 3: - train loss: 1.62 val loss: 1.56 - train acc: 0.68 val acc: 0.78:   7%|▋         | 2/30 [00:01<00:16,  1.71it/s]\n",
      "Epoch 3: - train loss: 1.62 val loss: 1.56 - train acc: 0.68 val acc: 0.78:  13%|█▎        | 4/30 [00:01<00:07,  3.41it/s]\n",
      "Epoch 3: - train loss: 1.61 val loss: 1.60 - train acc: 0.65 val acc: 0.61:  13%|█▎        | 4/30 [00:01<00:09,  2.69it/s]\n",
      "Epoch 3: - train loss: 1.58 val loss: 1.63 - train acc: 0.75 val acc: 0.64:  13%|█▎        | 4/30 [00:01<00:11,  2.25it/s]\n",
      "Epoch 3: - train loss: 1.58 val loss: 1.63 - train acc: 0.75 val acc: 0.64:  20%|██        | 6/30 [00:01<00:07,  3.36it/s]\n",
      "Epoch 3: - train loss: 1.61 val loss: 1.59 - train acc: 0.68 val acc: 0.66:  20%|██        | 6/30 [00:02<00:08,  2.89it/s]\n",
      "Epoch 3: - train loss: 1.54 val loss: 1.54 - train acc: 0.72 val acc: 0.75:  20%|██        | 6/30 [00:02<00:09,  2.53it/s]\n",
      "Epoch 3: - train loss: 1.54 val loss: 1.54 - train acc: 0.72 val acc: 0.75:  27%|██▋       | 8/30 [00:02<00:06,  3.37it/s]\n",
      "Epoch 3: - train loss: 1.61 val loss: 1.56 - train acc: 0.66 val acc: 0.74:  27%|██▋       | 8/30 [00:02<00:07,  3.01it/s]\n",
      "Epoch 3: - train loss: 1.51 val loss: 1.62 - train acc: 0.73 val acc: 0.62:  27%|██▋       | 8/30 [00:02<00:08,  2.71it/s]\n",
      "Epoch 3: - train loss: 1.51 val loss: 1.62 - train acc: 0.73 val acc: 0.62:  33%|███▎      | 10/30 [00:02<00:05,  3.38it/s]\n",
      "Epoch 3: - train loss: 1.56 val loss: 1.56 - train acc: 0.73 val acc: 0.74:  33%|███▎      | 10/30 [00:03<00:06,  3.08it/s]\n",
      "Epoch 3: - train loss: 1.53 val loss: 1.53 - train acc: 0.73 val acc: 0.70:  33%|███▎      | 10/30 [00:03<00:07,  2.83it/s]\n",
      "Epoch 3: - train loss: 1.53 val loss: 1.53 - train acc: 0.73 val acc: 0.70:  40%|████      | 12/30 [00:03<00:05,  3.39it/s]"
     ]
    }
   ],
   "source": [
    "scatternet_define_flags() \n",
    "scatternet_classifier = scatternet_train()  # no pretrained weights for the hybrid scatter net so we do not need to specify a k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE VISUALIZATION\n",
    "Sample some data points from MNIST to map to a feature space with both of our classifiers. Once we have both of these new feature spaces we can use TSNE to reduce dimensionality to a space easily visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_viz, y_viz = mnist.test.next_batch(1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE on 50 layer residual network features\n",
    "Looks like we are forming some clusters of each of the classes however there is heavy overlap. Multiple clusters are being formed for some of the classes. Some interesting future work could be a more in depth cluster analysis. There looks to be 3 different clusters for images representing 9s. Perhaps one cluster contains 9s that are straight, another of 9s slanted to the left, and a final cluster of 9s slanted to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); resnet_classifier_k.load() # have to save and load b/c of conflicting params between resnets\n",
    "\n",
    "resnet_k_features_viz = resnet_classifier_k.get_features(x_viz)\n",
    "visualize_features(resnet_k_features_viz, y_viz, 'resnet50_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE on unfrozen 50 layer residual network features\n",
    "It appears that we generate better separation between classes with the unfrozen residual network. I suspect that this is the case because the MNIST dataset did not impose a data constraint on us. However, if we were to significantly reduce access to data I suspect that the frozen residual network would provide better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); resnet_classifier_0.load() # have to save and load b/c of conflicting params between resnets\\\n",
    "\n",
    "resnet_0_features_viz = resnet_classifier_0.get_features(x_viz)\n",
    "visualize_features(resnet_0_features_viz, y_viz, 'resnet50_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE on Deep Hybrid Network\n",
    "Class separation is much stronger here than we experienced in our 50 layer residual network. I am guessing this is because the Deep Hybrid Network allowed for all the weights to be trained. While we froze all the weights and biases below the 3rd block of the resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatternet_features_viz = scatternet_classifier.get_features(x_viz)\n",
    "visualize_features(scatternet_features_viz, y_viz, 'scatternet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE CLASSIFICATION COMPARISON\n",
    "## Multilayer Neural Network vs. Logistic Regression\n",
    "The score function we use for comparison is just the mean number of labels correctly assigned to the test samples. The _viz features and labels used below were never used for training. The _viz features and labels were only used for TSNE visualizations and that is why we can still justify using them as our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(train_features, train_labels, test_features, test_labels):\n",
    "    lm = sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver='saga')\n",
    "    lm.fit(train_features, train_labels)\n",
    "    score = lm.score(test_features, test_labels)\n",
    "    return score\n",
    "\n",
    "def NN_score(nn, x_test, test_labels):\n",
    "    '''\n",
    "    nn here either represents the resnet or hybrid scatter network. Here we call them to generate a mean accuracy\n",
    "    \n",
    "    we don't submit the same test features here because the network is deterministic and will generate the same\n",
    "    intermediate features during the forward pass\n",
    "    '''\n",
    "    return nn.score(x_test, test_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train images and feature generation\n",
    "collect a batch of training images we can use as a training set for the logistic regression models. Extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = mnist.train.next_batch(1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Hybrid Scatter Network: logistic regression vs fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatternet_features_train = scatternet_classifier.get_features(x_train)\n",
    "\n",
    "lm_scatternet_score = logistic_regression(scatternet_features_train, y_train, scatternet_features_viz, y_viz)\n",
    "print('logistic regression on deep hybrid scatter network features accuracy: ', lm_scatternet_score)\n",
    "\n",
    "nn_scatternet_score = NN_score(scatternet_classifier, x_viz, y_viz)\n",
    "print('fully connected neural network stacked on scatter network features accuracy: ', nn_scatternet_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen block 4 resnet: logistic regression vs fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); resnet_classifier_k.load() # have to save and load b/c of conflicting params between resnets\n",
    "resnet_k_features_train = resnet_classifier_k.get_features(x_train)\n",
    "\n",
    "lm_res_k_score = logistic_regression(resnet_k_features_train, y_train, resnet_k_features_viz, y_viz)\n",
    "print('logistic regression on frozen resnet features accuracy: ', lm_res_k_score)\n",
    "\n",
    "nn_res_k_score = NN_score(resnet_classifier_k, x_viz, y_viz)\n",
    "print('fully connected neural network stacked on frozen resnet features accuracy: ', nn_res_4_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfrozen resnet: logistic regression vs fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph(); resnet_classifier_0.load() # have to save and load b/c of conflicting params between resnets\n",
    "resnet_0_features_train = resnet_classifier_0.get_features(x_train)\n",
    "\n",
    "lm_res_0_score = logistic_regression(resnet_0_features_train, y_train, resnet_0_features_viz, y_viz)\n",
    "print('logistic regression on unfrozen resnet features accuracy: ', lm_res_0_score)\n",
    "\n",
    "nn_res_0_score = NN_score(resnet_classifier_0, x_viz, y_viz)\n",
    "print('fully connected neural network stacked on unfrozen resnet features accuracy: ', nn_res_0_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "1. compared 6 different classifiers. They fell into a few different sub groups\n",
    "    1. Either features generated from a residual network or deep hybrid network\n",
    "    1. Either used a few frozen layers during training, initialized but left ufrozen, or began from a random initializer\n",
    "    c) The actual classification was either done with using a nonlinear function approximator or a linear function approximator\n",
    "        1. In our case the non linear function approximator was a fully connected neural network\n",
    "        1.) The linear function approximator was basic logistic regression. Our logistic regression model used saga for optimization. More information about saga can be read [here](https://www.di.ens.fr/~fbach/Defazio_NIPS2014.pdf) in a 2014 NIPs paper.\n",
    "1. The Unfrozen residual network and the deep hybrid network performed very similarly. It is unclear which of these two is actually supperior at this task. The Deep Hybrid Network performed marginally better, but hyperparams and preprocessing were optized for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
